{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Vl0RdKuDXzKn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym==0.25.2 in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym==0.25.2) (4.12.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym==0.25.2) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym==0.25.2) (1.23.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym==0.25.2) (0.0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym==0.25.2) (3.8.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym==0.25.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5-u_1J4X2nD"
   },
   "outputs": [],
   "source": [
    "import valuenetwork\n",
    "import experience_replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5JC32tLyX32y"
   },
   "outputs": [],
   "source": [
    "from valuenetwork import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_qbEeubX7N1"
   },
   "outputs": [],
   "source": [
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ly99hmNHX_NQ"
   },
   "outputs": [],
   "source": [
    "import valuenetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UIsnBW8lYAN0"
   },
   "outputs": [],
   "source": [
    "agent1= valuenetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDjVZLmvYAHm"
   },
   "outputs": [],
   "source": [
    "agent1.sqrt(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2WFbnIOmX_-e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be using GPU if the device has a GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"We will be using GPU if the device has a GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OI2kxaYoX_up"
   },
   "outputs": [],
   "source": [
    "pip install \"gym[atari, accept-rom-license]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmW0eor5YGam"
   },
   "source": [
    "#GPU CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ziwEZ-wfVI4E"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from itertools import count\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PBYBBD23YJpu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
      "/usr/local/lib/python3.9/dist-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/usr/local/lib/python3.9/dist-packages/keras/utils/image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "/usr/local/lib/python3.9/dist-packages/keras/utils/image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "/usr/local/lib/python3.9/dist-packages/keras/utils/image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "/usr/local/lib/python3.9/dist-packages/keras/utils/image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  'hamming': pil_image.HAMMING,\n",
      "/usr/local/lib/python3.9/dist-packages/keras/utils/image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  'box': pil_image.BOX,\n",
      "/usr/local/lib/python3.9/dist-packages/keras/utils/image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  'lanczos': pil_image.LANCZOS,\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('ppoPendulum_logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3r-hsKNmYPau"
   },
   "source": [
    "#Creating the policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4F9mg86pV5oB",
    "outputId": "ac3b0785-de34-4682-c12b-15b7120bc3d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v1')\n",
    "env.reset()\n",
    "num_of_ations = env.action_space\n",
    "num_of_actions=np.array(num_of_ations)\n",
    "num_of_actions.size\n",
    "print(env.observation_space.shape[0])\n",
    "state_space = env.observation_space.shape[0]\n",
    "print(state_space)\n",
    "hidden_layer1= 64\n",
    "hidden_layer2= 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fiHuUS2tVNDs"
   },
   "outputs": [],
   "source": [
    "class PPOPolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPOPolicyNetwork, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(state_space, hidden_layer1)\n",
    "        self.fc2 = nn.Linear(hidden_layer1, hidden_layer2)\n",
    "        self.fc_mu = nn.Linear(hidden_layer2, 1)\n",
    "        self.fc_std = nn.Linear(hidden_layer2, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = self.relu(self.fc1(state))\n",
    "        state = self.relu(self.fc2(state))\n",
    "        out = 2 * self.tanh(self.fc_mu(state))\n",
    "        std_dev = self.softplus(self.fc_std(state)) + 1e-3\n",
    "        return out, std_dev\n",
    "\n",
    "    def action_select(self, state):\n",
    "        with torch.no_grad():\n",
    "            out, std_dev = self.forward(state)\n",
    "            norm = Normal(out, std_dev)\n",
    "            action = norm.sample()\n",
    "        return np.clip(action.item(), -2., 2.)\n",
    "\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(state_space, hidden_layer1)\n",
    "        self.fc2 = nn.Linear(hidden_layer1, hidden_layer2)\n",
    "        self.fc3 = nn.Linear(hidden_layer2, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = self.relu(self.fc1(state))\n",
    "        state = self.relu(self.fc2(state))\n",
    "        state = self.fc3(state)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2rTmrivvVTVq",
    "outputId": "421c14bb-0f72-4d6d-a7f1-b9c0b23c35e3"
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1')\n",
    "policy = PPOPolicyNetwork().to(device)\n",
    "old_policy = PPOPolicyNetwork().to(device)\n",
    "optim = torch.optim.Adam(policy.parameters(), lr=1e-5)\n",
    "value = ValueNetwork().to(device)\n",
    "value_optim = torch.optim.Adam(value.parameters(), lr=2e-5)\n",
    "gamma = 0.9\n",
    "steps = 0\n",
    "\n",
    "is_learn = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dumFnP4RTuQL",
    "outputId": "43c83cb3-a8ac-4fda-c4c8-85733464f49d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3242/1575866667.py:33: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  state_tensor = torch.FloatTensor(states).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode number:0, episode reward is -1218.1000153185614\n",
      "Episode number:10, episode reward is -1419.519548661467\n",
      "Episode number:20, episode reward is -1132.4515411039733\n",
      "Episode number:30, episode reward is -1179.0698107561045\n",
      "Episode number:40, episode reward is -1479.862695721372\n",
      "Episode number:50, episode reward is -1491.0705469904706\n",
      "Episode number:60, episode reward is -1739.8063963580655\n",
      "Episode number:70, episode reward is -1198.6253045726291\n",
      "Episode number:80, episode reward is -1019.5810267323614\n",
      "Episode number:90, episode reward is -1619.0784393300328\n",
      "Episode number:100, episode reward is -1108.7041430069073\n",
      "Episode number:110, episode reward is -1605.2760106572596\n",
      "Episode number:120, episode reward is -1188.8014616468836\n",
      "Episode number:130, episode reward is -750.6670077548063\n",
      "Episode number:140, episode reward is -1033.1746531540368\n",
      "Episode number:150, episode reward is -974.1299381216464\n",
      "Episode number:160, episode reward is -1199.2097228216626\n",
      "Episode number:170, episode reward is -1418.2900624738224\n",
      "Episode number:180, episode reward is -1229.1897074335718\n",
      "Episode number:190, episode reward is -1322.371959243083\n",
      "Episode number:200, episode reward is -1084.0922438220457\n",
      "Episode number:210, episode reward is -885.7126105188268\n",
      "Episode number:220, episode reward is -1133.745144805249\n",
      "Episode number:230, episode reward is -1314.1485591275798\n",
      "Episode number:240, episode reward is -1509.3967556106766\n",
      "Episode number:250, episode reward is -1070.486644867868\n",
      "Episode number:260, episode reward is -1090.4464714718729\n",
      "Episode number:270, episode reward is -753.326922009331\n",
      "Episode number:280, episode reward is -979.4326744810722\n",
      "Episode number:290, episode reward is -869.7999484702349\n",
      "Episode number:300, episode reward is -964.6986520968285\n",
      "Episode number:310, episode reward is -1185.329001615716\n",
      "Episode number:320, episode reward is -1069.5595176549696\n",
      "Episode number:330, episode reward is -873.6939535405932\n",
      "Episode number:340, episode reward is -1008.087134249617\n",
      "Episode number:350, episode reward is -929.5713102970334\n",
      "Episode number:360, episode reward is -1503.383775019741\n",
      "Episode number:370, episode reward is -935.9638163405069\n",
      "Episode number:380, episode reward is -897.1538207031061\n",
      "Episode number:390, episode reward is -1007.6763712954372\n",
      "Episode number:400, episode reward is -902.6275618128348\n",
      "Episode number:410, episode reward is -771.114997245155\n",
      "Episode number:420, episode reward is -911.1631668327263\n",
      "Episode number:430, episode reward is -759.8408120921647\n",
      "Episode number:440, episode reward is -1083.017803565334\n",
      "Episode number:450, episode reward is -512.2098864332414\n",
      "Episode number:460, episode reward is -641.779155761963\n",
      "Episode number:470, episode reward is -1057.6198102498072\n",
      "Episode number:480, episode reward is -937.9115243324054\n",
      "Episode number:490, episode reward is -1147.821078070794\n",
      "Episode number:500, episode reward is -909.6552082318406\n",
      "Episode number:510, episode reward is -721.6378760254161\n",
      "Episode number:520, episode reward is -1254.425362894406\n",
      "Episode number:530, episode reward is -496.02062014279863\n",
      "Episode number:540, episode reward is -921.4096098836148\n",
      "Episode number:550, episode reward is -517.3664601666088\n",
      "Episode number:560, episode reward is -859.8347583960305\n",
      "Episode number:570, episode reward is -638.1219254317391\n",
      "Episode number:580, episode reward is -392.3718532547985\n",
      "Episode number:590, episode reward is -909.8660101157554\n",
      "Episode number:600, episode reward is -523.0117593547993\n",
      "Episode number:610, episode reward is -860.1230441716266\n",
      "Episode number:620, episode reward is -481.0421658344407\n",
      "Episode number:630, episode reward is -651.5229969015138\n",
      "Episode number:640, episode reward is -520.8337857024738\n",
      "Episode number:650, episode reward is -958.7847567982759\n",
      "Episode number:660, episode reward is -687.0769069389801\n",
      "Episode number:670, episode reward is -528.5449631448396\n",
      "Episode number:680, episode reward is -396.12574634645387\n",
      "Episode number:690, episode reward is -810.7216270322572\n",
      "Episode number:700, episode reward is -733.7345372232398\n",
      "Episode number:710, episode reward is -1277.853837852041\n",
      "Episode number:720, episode reward is -1078.4057804122222\n",
      "Episode number:730, episode reward is -1352.186200271786\n",
      "Episode number:740, episode reward is -1146.748567305298\n",
      "Episode number:750, episode reward is -894.9242058106944\n",
      "Episode number:760, episode reward is -507.80893553697376\n",
      "Episode number:770, episode reward is -801.8708828950599\n",
      "Episode number:780, episode reward is -712.3618991813278\n",
      "Episode number:790, episode reward is -257.9428707894807\n",
      "Episode number:800, episode reward is -581.7592925476768\n",
      "Episode number:810, episode reward is -903.8234793951674\n",
      "Episode number:820, episode reward is -400.4460401564943\n",
      "Episode number:830, episode reward is -535.5526438445177\n",
      "Episode number:840, episode reward is -261.4930495994663\n",
      "Episode number:850, episode reward is -703.886727197033\n",
      "Episode number:860, episode reward is -661.750201168569\n",
      "Episode number:870, episode reward is -1212.9276199019125\n",
      "Episode number:880, episode reward is -525.3777143696202\n",
      "Episode number:890, episode reward is -1226.7619339881815\n",
      "Episode number:900, episode reward is -397.39479042024936\n",
      "Episode number:910, episode reward is -423.3830529783436\n",
      "Episode number:920, episode reward is -812.1094620686423\n",
      "Episode number:930, episode reward is -405.8332932614283\n",
      "Episode number:940, episode reward is -793.3947041886953\n",
      "Episode number:950, episode reward is -1085.3961727550281\n",
      "Episode number:960, episode reward is -978.8236279802707\n",
      "Episode number:970, episode reward is -1357.2998747383326\n",
      "Episode number:980, episode reward is -1265.2299930209588\n",
      "Episode number:990, episode reward is -130.6059804688724\n",
      "Episode number:1000, episode reward is -421.320243634507\n",
      "Episode number:1010, episode reward is -270.39465026361944\n",
      "Episode number:1020, episode reward is -130.84204728933324\n",
      "Episode number:1030, episode reward is -1265.7042051110932\n",
      "Episode number:1040, episode reward is -277.7013591788788\n",
      "Episode number:1050, episode reward is -1184.4972452826755\n",
      "Episode number:1060, episode reward is -1250.8273928000815\n",
      "Episode number:1070, episode reward is -411.12724868190213\n",
      "Episode number:1080, episode reward is -942.6196996591812\n",
      "Episode number:1090, episode reward is -1021.9803964688454\n",
      "Episode number:1100, episode reward is -1392.3852954797987\n",
      "Episode number:1110, episode reward is -1157.9649695048417\n",
      "Episode number:1120, episode reward is -1092.6114999495812\n",
      "Episode number:1130, episode reward is -1170.3061975841813\n",
      "Episode number:1140, episode reward is -1071.1546328355003\n",
      "Episode number:1150, episode reward is -1259.315215423388\n",
      "Episode number:1160, episode reward is -599.7222913177674\n",
      "Episode number:1170, episode reward is -1195.9690314684951\n",
      "Episode number:1180, episode reward is -938.9711600525294\n",
      "Episode number:1190, episode reward is -267.12805005650404\n",
      "Episode number:1200, episode reward is -763.2751137082746\n",
      "Episode number:1210, episode reward is -796.6436892718798\n",
      "Episode number:1220, episode reward is -399.97322217727526\n",
      "Episode number:1230, episode reward is -1.996306515268621\n",
      "Episode number:1240, episode reward is -688.4936756944737\n",
      "Episode number:1250, episode reward is -940.7219024687862\n",
      "Episode number:1260, episode reward is -661.2578423039039\n",
      "Episode number:1270, episode reward is -129.95089350652\n",
      "Episode number:1280, episode reward is -402.7979771157011\n",
      "Episode number:1290, episode reward is -129.978894034616\n",
      "Episode number:1300, episode reward is -402.9492851946061\n",
      "Episode number:1310, episode reward is -133.5952547172794\n",
      "Episode number:1320, episode reward is -1.459781773251874\n"
     ]
    }
   ],
   "source": [
    "for ep in count():\n",
    "    state = env.reset()\n",
    "    ep_reward = 0\n",
    "    rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    total_reward_for_graph=[]\n",
    "    for time_steps in range(200):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action = policy.action_select(state_tensor)\n",
    "        next_state, reward, done, _ = env.step([action])\n",
    "        ep_reward += reward\n",
    "        total_reward_for_graph.append(ep_reward)\n",
    "        reward = (reward + 8.1) / 8.1\n",
    "\n",
    "        rewards.append(reward)\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if (time_steps+1) % 32 == 0 or time_steps == 199:\n",
    "            old_policy.load_state_dict(policy.state_dict())\n",
    "            with torch.no_grad():\n",
    "                next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "                R = value(next_state_tensor)\n",
    "            for i in reversed(range(len(rewards))):\n",
    "                R = gamma * R + rewards[i]\n",
    "                rewards[i] = R\n",
    "            rewards_tensor = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "            for K in range(10):\n",
    "                steps += 1\n",
    "                state_tensor = torch.FloatTensor(states).to(device)\n",
    "                action_tensor = torch.FloatTensor(actions).unsqueeze(1).to(device)\n",
    "                with torch.no_grad():\n",
    "                    advantage = rewards_tensor - value(state_tensor)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    old_out, old_std_dev = old_policy(state_tensor)\n",
    "                    old_norm = Normal(old_out, old_std_dev)\n",
    "               \n",
    "                out, std_dev= policy(state_tensor)\n",
    "                \n",
    "                norm = Normal(out, std_dev)\n",
    "                log_prob = norm.log_prob(action_tensor)\n",
    "                old_log_prob = old_norm.log_prob(action_tensor)\n",
    "                ratio = torch.exp(log_prob - old_log_prob)\n",
    "               \n",
    "                L1 = ratio * advantage\n",
    "                L2 = torch.clamp(ratio, 0.8, 1.2) * advantage\n",
    "               \n",
    "                loss = torch.min(L1, L2)\n",
    "                loss = - loss.mean()\n",
    "                \n",
    "                \n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                value_loss = F.mse_loss(rewards_tensor, value(state_tensor))\n",
    "                value_optim.zero_grad()\n",
    "                value_loss.backward()\n",
    "                value_optim.step()\n",
    "            rewards = []\n",
    "            states = []\n",
    "            actions = []\n",
    "    writer.add_scalar('episode reward', ep_reward, ep)\n",
    "    if ep % 10 == 0:\n",
    "        print('Episode number:{}, episode reward is {}'.format(ep, ep_reward))\n",
    "        torch.save(policy.state_dict(), 'ppo-policy_for_pend.para')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "jQFPry08duhk",
    "outputId": "9ac9a769-b510-4fb7-a14c-4d2fec036f1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0bca396810>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(total_reward_for_graph)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
